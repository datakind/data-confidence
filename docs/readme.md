# The Community Health IoP Cookbook  

## Who this guide is for  

This guide is for creators and implementors of data products for community health volunteers (CHVs) to gather and act on data from patients in their community. CHVs perform critical front-line medical work in places where health infrastructure and resources are frequently lacking. They work under challenging conditions, with diverse populations, and often limited equipment. For these reasons, those seeking to analyze the data generated by CHVs regularly find that they're unable to sufficiently trust it.

For this reason, we have developed a suite of tools to test, document, and analyze the "inconsistent or problematic" data points (IOP) appearing in a community health data collection implementation. While this code base was designed and implemented specifically to support the tools built and maintained by Medic Mobile in support of CHVs in Siaya County, Kenya, our hope is that they can be easily deployed by other mobile health datasets with minimial modifications.

This guide aims to provide some instructions, examples, and troubleshooting tips for anyone seeking to implement this toolset on their own community health setup. Since community health tools can vary widely in both their underlying construction as well as their specific implementations in the field, we've opted to take a "cookbook" approach - that is, providing some representative examples aimed at demonstrating the various options for detecting and documenting various different types of IOP so that users of these tools can easily modify them to meet their particular needs.

## The basics

This toolset is setup to run tests on mobile health data to spot potential IOP in the same way that one might write and run a set of unit tests to verify the correctness of a software program. We use two different tools to support different types of tests:

- [DBT](./dbt.md): DBT is a powerful data engineering tool that supports one-line setup of many common tests along with support for any test that can be written as a SQL statement. We recommend first considering implementing all tests in DBT, before trying other tools if your test doesn't make sense in a SQL paradigm
- [Great Expectations](./great_expectations.md): For situations that don't lend themselves well to SQL (particularly statistics-based tests), we support testing using Great Expectations. Great Expectations are written as python expressions, with many common statistical tests available as one-liners out of the box. Great Expectations also allows execution of arbitrary Pandas code, which allows for flexibility when tests require more complex data manipulations. However, since these situations are particularly bespoke to a given implementation, we provide only basic guidance in this documentation.

## Installation and Getting Started

- [Getting Started](./getting_started.md)

## Learning More

- [Examples of different kinds of tests](./examples.md)
- [Implementing tests in DBT](./dbt.md)
- [Implementing tests in Great Expectations](./great_expectations.md)

## Implementing in Production

At this stage of the work, we've left specific decisions about how to implement this work in production systems up to end users. Since each organization's technology stack, data infrastructure, and tools of choice are likely to be different, we don't want to presume we know what's best for a given organization. That said, there are a few notes to be aware of.

Both DBT and Great Expectations support a wide variety of underlying databases, but we have developed and tested solely on postgres. Others may work with varying degrees of modifications needed, but we can't speak to the specifics of any other databases in particular. Naturally, if you're considering using other databases, make sure they're supported by both DBT and Great Expectations or otherwise you'll only be able to run parts of this stack

We assume that most users will want to orchestrate the running of the pipelines in here via some form of workflow tool. This could be as simple as running `run_everything.sh` on a standard cronjob every 24 hours. Alternatively, one might use a tool like Airflow or Prefect to run specific tests under specific timing and complex conditions. Either way, all the scripts we've provided here are either python or bash and should integrae nicely with this type of common workflow tools. In addition, DBT and Great Expectations commands to perform specific operations (like `dbt test -m core`) should also work well with orchestration tools since they're designed to be invoked via the command line already.

## Other things to remember  

- installation/windows
- dependencies in terms of downstream tests and reports

### I don't know which paradigm I should use for my test

If you're not sure, start with DBT. If you're doing something that requires looking at distributions of data in columns, consider Great Expectations (unless you really like writing complicated SQL). It's also possible that you'll want to merge paradigms to test particularly complex situations. For example, you might create a sql-based DBT model, read data from it in Great Expectations and perform some additional manipulations in Pandas, then use a statistics-based expectation in Great Expectations to determine the ultimate set of IoP results.
